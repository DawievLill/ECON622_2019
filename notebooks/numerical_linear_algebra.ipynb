{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id='numerical-linear-algebra'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Linear Algebra and Factorizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [Numerical Linear Algebra and Factorizations](#Numerical-Linear-Algebra-and-Factorizations)  \n",
    "  - [Overview](#Overview)  \n",
    "  - [Simple Examples](#Simple-Examples)  \n",
    "  - [Factorizations](#Factorizations)  \n",
    "  - [Continuous Time Markov Chains (CTMC)](#Continuous-Time-Markov-Chains-%28CTMC%29)  \n",
    "  - [Banded Matrices](#Banded-Matrices)  \n",
    "  - [Implementing Low Level Kernels](#Implementing-Low-Level-Kernels)  \n",
    "  - [Exercises](#Exercises)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You cannot learn too much linear algebra. – Benedict Gross"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In particular, we will examine the structure of matrices and linear operators (e.g., dense, sparse, symmetric, tridiagonal, banded) and\n",
    "discuss how it can be exploited to radically increase the performance of solving the problems.\n",
    "\n",
    "We build on [linear algebra](linear_algebra.html), [orthogonal projections](orth_proj.html), and [finite Markov Chains](finite_markov.html).\n",
    "\n",
    "The methods in this section are called direct methods, and they are qualitatively similar to performing Gaussian elimination to factor matrices and solve systems.  In [iterative methods and sparsity](iterative_methods_sparsity.html) we examine a different approach with iterative algorithms, and generalized the matrices as linear operators.y\n",
    "\n",
    "The list of specialized packages for these tasks is enormous and growing, but some of the important organizations to\n",
    "look at are [JuliaMatrices](https://github.com/JuliaMatrices) , [JuliaSparse](https://github.com/JuliaSparse), and [JuliaMath](https://github.com/JuliaMath)\n",
    "\n",
    "**NOTE** This lecture explores techniques linear algebra, with an emphasis on large systems.  You may wish to review multiple-dispatch and generic programming in  introduction to types, and consider further study on [generic programming](more_julia/generic_programming.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hide-output": true
   },
   "outputs": [],
   "source": [
    "using LinearAlgebra, Statistics, BenchmarkTools, SparseArrays, Random\n",
    "Random.seed!(42);  # seed random numbers for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications\n",
    "\n",
    "Some key questions to motivate the lecture.  Is the following a computationally expensive operation as the size of the matrix increases?\n",
    "\n",
    "- Multiplying two matrices?  It depends.  Multiplying 2 diagonals is trivial.  \n",
    "- Solving a linear system of equations?  It depends.  If the matrix is the identity, the solution is the vector itself.  \n",
    "- Finding the eigenvalues of a matrix?  It depends.  The eigenvalues of a triangular matrix are the diagonal.  \n",
    "\n",
    "\n",
    "With that in mind, in this section lecture, we consider variations on three classic problems.\n",
    "\n",
    "First is the solution to\n",
    "\n",
    "$$\n",
    "A x = b\n",
    "$$\n",
    "\n",
    "for a square $ A $ where we will maintain throughout there is a unique solution.\n",
    "\n",
    "On paper, since the [Invertible Matrix Theorem](https://en.wikipedia.org/wiki/Invertible_matrix#The_invertible_matrix_theorem) tells us a unique solution is\n",
    "equivalent to $ A $ being invertible, we often write the solution as\n",
    "\n",
    "$$\n",
    "x = A^{-1} b\n",
    "$$\n",
    "\n",
    "Second, in the case of a rectangular matrix, $ A $ we consider the [linear least-squares](https://en.wikipedia.org/wiki/Linear_least_squares) solution\n",
    "to\n",
    "\n",
    "$$\n",
    "\\min_x ||Ax -b||^2\n",
    "$$\n",
    "\n",
    "From theory, we know that $ A $ has linearly independent columns that the solution is the [normal equations](https://en.wikipedia.org/wiki/Linear_least_squares#Derivation_of_the_normal_equations)\n",
    "\n",
    "$$\n",
    "x = (A'A)^{-1}A'b\n",
    "$$\n",
    "\n",
    "And finally, consider the eigenvalue problem of finding $ x $ and $ \\lambda $ such that\n",
    "\n",
    "$$\n",
    "A x = \\lambda x\n",
    "$$\n",
    "\n",
    "For the eigenvalue problems.  Keep in mind that that you do not always require all of the $ \\lambda $, and sometimes the largest (or smallest) would be enough.  For example, calculating the spectral radius only requires the maximum eigenvalue in absolute value.\n",
    "\n",
    "The theme of this lecture, and numerical linear algebra in general, comes down to three principles:\n",
    "\n",
    "1. **identify structure** (e.g. [symmetric, sparse, diagonal,etc.](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/index.html#Special-matrices-1)) of $ A $ in order to use **specialized algorithms**  \n",
    "1. **do not lose structure** by applying the wrong linear algebra operations at the wrong times (e.g. sparse matrix becoming dense)  \n",
    "1. understand the **computational complexity** of each algorithm  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Complexity\n",
    "\n",
    "As the goal of this section is to move towards numerical linear algebra of large systems, we need to understand how well algorithms scale with size.  This notion is called [computational complexity](https://en.wikipedia.org/wiki/Computational_complexity).\n",
    "\n",
    "While this notion of complexity can work at various levels such as the number of [significant digits](https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations#Arithmetic_functions) for basic mathematical operations, the amount of memory and storage required, or the amount of time) - but we will typically focus on the time-complexity.\n",
    "\n",
    "For time-complexity, the `N` is usually the dimensionality of the problem, although occasionally the key will be the number of non-zeros in the matrix or width of bands.  For our applications, time-complexity is best thought of as the number of floating point operations (e.g. add, multiply, etc.) required.\n",
    "\n",
    "Complexity of algorithms is typically written in [Big O](https://en.wikipedia.org/wiki/Big_O_notation) notation which provides bounds on the scaling.\n",
    "\n",
    "Formally, we can write this as $ f(N) = O(g(N)) \\text{ as} N \\to \\infty $ wher the interpretation is that there exists some constants $ M, N_0 $ such that\n",
    "\n",
    "$$\n",
    "f(N) \\leq M g(N), \\text{ for } N > N_0\n",
    "$$\n",
    "\n",
    "For example, the complexity of finding an LU Decomposition of a dense matrix is $ O(N^3) $ which should be read as there being a constant where\n",
    "eventually the number of floating point operations required decompose a matrix of size $ N\\times N $ grows cubically.\n",
    "\n",
    "Keep in mind that these are asymptotic results intended to understanding the scaling of the problem, and the constant can matter for a given\n",
    "fixed size.\n",
    "\n",
    "For example, the number of operations required for an [LU decomposition](https://en.wikipedia.org/wiki/LU_decomposition#Algorithms) of a dense $ N \\times N $ matrix $ 2/3 N^3 $, ignoring the $ N^2 $ and lower terms.  However, sparse matrix algorithms instead scale with the number of non-zeros in the matrix instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rules of Computational Complexity\n",
    "\n",
    "When combining algorithms, you will sometimes need to think through how [combining algorithms](https://en.wikipedia.org/wiki/Big_O_notation#Properties) changes complexity.  For example, if you do,\n",
    "\n",
    "1. an $ O(N^3) $ operation $ P $ times, then it simply changes the constant and remains $ O(N^3) $  \n",
    "1. one $ O(N^3) $ operation and another $ O(N^2) $ one, then you take the max, which does not change the scale $ O(N^3) $  \n",
    "1. a repetition of a $ O(N) $ operation that itself uses an $ O(N) $ one, you take the product, and the complexity becomes $ O(N^2) $  \n",
    "\n",
    "\n",
    "With this, there is a word of caution: dense matrix-multiplication is an [expensive operation](https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations#Matrix_algebra) for unstructured matrices, and the basic version is $ O(N^3) $.\n",
    "\n",
    "Of course, modern libraries use highly turned and [careful algorithms](https://en.wikipedia.org/wiki/Matrix_multiplication_algorithm) to multiply matrices and exploit the computer architecture, memory cache, etc., but this simply lowers the constant of proportionality and they remain $ O(N^3) $.\n",
    "\n",
    "A consequence is that, since many algorithms require matrix-matrix multiplication, it means that it is usually not possible to go below that order without further matrix structure.\n",
    "\n",
    "That is, changing the constant of proportionality for a given size can help, but in order to achieve higher scaling you need to identify matrix structure and ensure your operations do not lose it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losing Structure\n",
    "\n",
    "As a first example of a structured matrix, consider a [sparse arrays](https://docs.julialang.org/en/v1/stdlib/SparseArrays/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nnz(A) = 47\n",
      "nnz(invA) = 100\n"
     ]
    }
   ],
   "source": [
    "A = sprand(10, 10, 0.45)  # random 10x10, 45% filled with non-zeros\n",
    "\n",
    "@show nnz(A)  # counts the number of non-zeros\n",
    "invA = sparse(inv(Array(A)))  # Julia won't even invert sparse directly\n",
    "@show nnz(invA);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This increase from 45 to 100 percent dense demonstrates that significant sparsity can be lost when calculating an inverse.\n",
    "\n",
    "The results can be even more extreme.  Consider a tridiagonal matrix of size $ N \\times N $\n",
    "that might come out of a Markov Chain or discretization  of a diffusion process,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Tridiagonal{Float64,Array{Float64,1}}:\n",
       " 0.8  0.2   ⋅    ⋅    ⋅ \n",
       " 0.1  0.8  0.1   ⋅    ⋅ \n",
       "  ⋅   0.1  0.8  0.1   ⋅ \n",
       "  ⋅    ⋅   0.1  0.8  0.1\n",
       "  ⋅    ⋅    ⋅   0.2  0.8"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 5\n",
    "A = Tridiagonal([fill(0.1, N-2); 0.2], fill(0.8, N), [0.2; fill(0.1, N-2);])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of non-zeros here is approximately $ 3 N $, linear, which scales well for huge matrices into the millions or billions\n",
    "\n",
    "But consider the inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Array{Float64,2}:\n",
       "  1.29099      -0.327957     0.0416667  -0.00537634   0.000672043\n",
       " -0.163978      1.31183     -0.166667    0.0215054   -0.00268817 \n",
       "  0.0208333    -0.166667     1.29167    -0.166667     0.0208333  \n",
       " -0.00268817    0.0215054   -0.166667    1.31183     -0.163978   \n",
       "  0.000672043  -0.00537634   0.0416667  -0.327957     1.29099    "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the matrix is fully dense and scales $ N^2 $\n",
    "\n",
    "This also applies to the $ A' A $ operation in the normal equations of LLS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nnz(A) / 20 ^ 2 = 0.2825\n",
      "nnz(A' * A) / 21 ^ 2 = 0.800453514739229\n"
     ]
    }
   ],
   "source": [
    "A = sprand(20, 21, 0.3)\n",
    "@show nnz(A)/20^2\n",
    "@show nnz(A'*A)/21^2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there is some variation based on the randoms chosen, we see that a 30 percent dense matrix becomes almost full dense\n",
    "after the product is taken.\n",
    "\n",
    "**Sparsity/Structure is not just for storage**:  While we have been emphasizing counting the non-zeros as a heuristic, the primary reason to maintain structure\n",
    "and sparsity is not for using less memory to store the matrices.\n",
    "\n",
    "Size can sometimes become important (e.g. a 1 million by 1 million tridiagonal matrix needs to store 3 million numbers (i.e, about 6MB of memory), where a dense one requires 1 trillion (i.e., about 1TB of memory).\n",
    "\n",
    "But, as we will see, the main purpose of considering sparsity and matrix structure is that it enables specialized algorithms which typically\n",
    "have a lower-computational order than unstructured dense, or even an unstructured sparse operations.\n",
    "\n",
    "First, create convenient functions for benchmarking which displays the type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "benchmark_solve (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using BenchmarkTools\n",
    "function benchmark_solve(A, b)\n",
    "    println(\"A\\\\b for typeof(A) = $(string(typeof(A)))\")\n",
    "    @btime $A \\ $b\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, take away structure to see the impact on performance,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\\b for typeof(A) = Tridiagonal{Float64,Array{Float64,1}}\n",
      "  20.600 μs (9 allocations: 47.75 KiB)\n",
      "A\\b for typeof(A) = SparseMatrixCSC{Float64,Int64}\n",
      "  535.700 μs (69 allocations: 1.06 MiB)\n",
      "A\\b for typeof(A) = Array{Float64,2}\n",
      "  45.582 ms (5 allocations: 7.65 MiB)\n"
     ]
    }
   ],
   "source": [
    "N = 1000\n",
    "b = rand(N)\n",
    "A = Tridiagonal([fill(0.1, N-2); 0.2], fill(0.8, N), [0.2; fill(0.1, N-2);])\n",
    "A_sparse = sparse(A)\n",
    "A_dense = Array(A)\n",
    "\n",
    "# benchmark solution to system A x = b and A * B\n",
    "benchmark_solve(A, b)\n",
    "benchmark_solve(A_sparse, b)\n",
    "benchmark_solve(A_dense, b);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows what is at stake:  using a structured tridiagonal may be 10-20x faster than using a sparse matrix which is 100x faster then\n",
    "using a dense matrix.\n",
    "\n",
    "In fact, the difference becomes more extreme as the matrices grow.  Solving a tridiagonal system is an $ O(N) $ while that of a dense matrix without any structure is $ O(N^3) $.  The complexity of a sparse solution is more complicated, but roughly scales by $ O(nnz(N)) $, i.e. the number of nonzeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverting Matrices\n",
    "\n",
    "To begin, consider a simple linear system of a dense matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Float64,1}:\n",
       " 0.5125312396664188\n",
       " 0.6828679275339775\n",
       " 0.3454193757566306\n",
       " 0.8706954827549864"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 4\n",
    "A = rand(N,N)\n",
    "b = rand(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On paper, we to solve for $ A x = b $ by inverting the matrix,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Float64,1}:\n",
       " -0.8230616861035331\n",
       "  1.1336956414831805\n",
       " -1.642325023331038 \n",
       "  2.2104913647288558"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = inv(A) * b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we will see, inverting matrices should be used for theory, not for code.  The classic advice that you should [never invert a matrix](https://www.johndcook.com/blog/2010/01/19/dont-invert-that-matrix) may be [slightly exaggerated](https://arxiv.org/abs/1201.6035), but is generally good advice.  In fact, the methods used by libraries to invert matrices typically calculate the same factorizations used for computing a system of equations.\n",
    "\n",
    "To summarize the wisdom: solving a system by inverting a matrix is always a little slower, potentially less accurate, and will often lose crucial sparsity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triangular Matrices and Back/Forward Substitution\n",
    "\n",
    "To begin, consider solving a system with an `UpperTriangular` matrix,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 UpperTriangular{Float64,Array{Float64,2}}:\n",
       " 1.0  2.0  3.0\n",
       "  ⋅   5.0  6.0\n",
       "  ⋅    ⋅   9.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = [1.0, 2.0, 3.0]\n",
    "U = UpperTriangular([1.0 2.0 3.0; 0.0 5.0 6.0; 0.0 0.0 9.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This system is especially easy to solve using [back-substitution](https://en.wikipedia.org/wiki/Triangular_matrix#Forward_and_back_substitution).  In particular, $ x_3 = b_3 / U_{33}, x_2 = (b_2 - x_3 U_{23})/U_{22} $, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " 0.0               \n",
       " 0.0               \n",
       " 0.3333333333333333"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U \\ b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `LowerTriangular` has similar properties and can be solved with forward-substitution.  For these matrices, no further matrix factorization is needed.\n",
    "\n",
    "The computational order of back-substitution and forward-substitution is $ O(N^2) $ for dense matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Warning on Matrix Multiplication\n",
    "\n",
    "Why we write matrix multiplications in our algebra with abandon, in practice the operation scales very poorly without any matrix structure.\n",
    "\n",
    "Matrix multiplication is so important to modern computers that the constant of scaling in front of the scaling has been radically reduced\n",
    "when using a proper package, but the order is still $ O(N^3) $ in practice.\n",
    "\n",
    "Sparse matrix multiplication, on the other hand, is $ O(N M_A M_B) $ where $ M_A $ are the number of nonzeros per row of $ A $ and $ B $ are the number of non-zeros per column of $ B $.\n",
    "\n",
    "By the rules of computational order, that means any algorithm this means that any algorithm requiring a matrix multiplication of dense matrices requires at least $ O(N^3) $ operation.\n",
    "\n",
    "The other important question is what is the structure of the resulting matrix. As always, we want to avoid losing\n",
    "\n",
    "For example, multiplying an upper triangular by a lower triangular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 UpperTriangular{Float64,Array{Float64,2}}:\n",
       " 0.409653  0.0195282  0.87974     0.552949  0.252486\n",
       "  ⋅        0.436677   0.662624    0.736553  0.224344\n",
       "  ⋅         ⋅         0.00653798  0.649019  0.134132\n",
       "  ⋅         ⋅          ⋅          0.168964  0.834989\n",
       "  ⋅         ⋅          ⋅           ⋅        0.708384"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 5\n",
    "U = UpperTriangular(rand(N,N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Adjoint{Float64,UpperTriangular{Float64,Array{Float64,2}}}:\n",
       " 0.409653   0.0       0.0         0.0       0.0     \n",
       " 0.0195282  0.436677  0.0         0.0       0.0     \n",
       " 0.87974    0.662624  0.00653798  0.0       0.0     \n",
       " 0.552949   0.736553  0.649019    0.168964  0.0     \n",
       " 0.252486   0.224344  0.134132    0.834989  0.708384"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = U'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the multiplication is fully dense (e.g. think of a cholesky multiplied by itself to produce a covariance matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Array{Float64,2}:\n",
       " 0.167816    0.00799978  0.360388  0.226517  0.103432\n",
       " 0.00799978  0.191068    0.306533  0.332434  0.102897\n",
       " 0.360388    0.306533    1.21306   0.978752  0.371655\n",
       " 0.226517    0.332434    0.978752  1.29804   0.532991\n",
       " 0.103432    0.102897    0.371655  0.532991  1.33109 "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L * U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, a tridiagonal times a diagonal is still a tridiagonal and $ O(N^2) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Tridiagonal{Float64,Array{Float64,1}}:\n",
       " 0.561922   0.14048     ⋅          ⋅          ⋅      \n",
       " 0.0568224  0.454579   0.0568224   ⋅          ⋅      \n",
       "  ⋅         0.0402454  0.321963   0.0402454   ⋅      \n",
       "  ⋅          ⋅         0.01826    0.14608    0.01826 \n",
       "  ⋅          ⋅          ⋅         0.0123203  0.049281"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = Tridiagonal([fill(0.1, N-2); 0.2], fill(0.8, N), [0.2; fill(0.1, N-2);])\n",
    "D = Diagonal(rand(N))\n",
    "D * A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factorizations\n",
    "\n",
    "When you tell a numerical analyst you are solving a linear system directly, their first question is “which factorization?”\n",
    "\n",
    "\n",
    "<a id='jl-decomposition'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LU Decomposition\n",
    "\n",
    "For a general dense matrix without any other structure (i.e. not known to be symmetric, tridiagonal, etc.) the standard approach is to\n",
    "factor the matrix iin order to exploit the speed of backward and forward substitution to complete the solution.\n",
    "\n",
    "The computational order of LU decomposition for a dense matrix is $ O(N^3) $ - the same as Gaussian elimination, but it tends\n",
    "to have a better constant term than others (e.g. half the number of operations of the QR).  For structured matrices\n",
    "or sparse ones, that order drops.\n",
    "\n",
    "The $ LU $ decompositions finds a lower triangular $ L $ and upper triangular $ U $ such that $ L U = A $.\n",
    "\n",
    "We can see which algorithm Julia will use for the `\\` operator by looking at the `factorize` function for a given\n",
    "matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LU{Float64,Array{Float64,2}}\n",
       "L factor:\n",
       "4×4 Array{Float64,2}:\n",
       " 1.0       0.0       0.0       0.0\n",
       " 0.933578  1.0       0.0       0.0\n",
       " 0.613678  0.551771  1.0       0.0\n",
       " 0.686831  0.729438  0.932112  1.0\n",
       "U factor:\n",
       "4×4 Array{Float64,2}:\n",
       " 0.916223  0.0915758  0.221304   0.0908984\n",
       " 0.0       0.712447   0.083115   0.681078 \n",
       " 0.0       0.0        0.806706   0.239325 \n",
       " 0.0       0.0        0.0       -0.452328 "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 4\n",
    "A = rand(N,N)\n",
    "b = rand(N)\n",
    "\n",
    "Af = factorize(A)  # chooses the right factorization, LU here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, it provides an $ L $ and $ U $ factorization (with [pivoting](https://en.wikipedia.org/wiki/LU_decomposition#LU_factorization_with_full_pivoting) ).\n",
    "\n",
    "With the factorization complete, we can solve different `b` right hand sides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Float64,1}:\n",
       "  0.06909902818646778\n",
       " -0.453743557563131  \n",
       " -0.11556849117372223\n",
       "  1.4306380720047653 "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b2 = rand(N)\n",
    "Af \\ b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This decomposition also includes a $ P $ is a [permutation matrix](https://en.wikipedia.org/wiki/Permutation_matrix) such\n",
    "that $ P A = L U $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Af.P * A ≈ Af.L * Af.U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also directly calculate an `lu` decomposition without the pivoting,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LU{Float64,Array{Float64,2}}\n",
       "L factor:\n",
       "4×4 Array{Float64,2}:\n",
       " 1.0         0.0      0.0     0.0\n",
       " 0.735697    1.0      0.0     0.0\n",
       " 0.657339   16.8663   1.0     0.0\n",
       " 1.07115   171.134   10.8358  1.0\n",
       "U factor:\n",
       "4×4 Array{Float64,2}:\n",
       " 0.855366   0.79794       0.28972    0.765939\n",
       " 0.0       -0.00445928    0.751419  -0.233514\n",
       " 0.0        0.0         -11.8757     4.10594 \n",
       " 0.0        0.0           0.0       -5.25829 "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L, U = lu(A, Val(false))  # the Val(false) provides solution without permutation matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can verify the decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A ≈ L * U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see roughly how the solver works, note that we can write the problem $ A x = b $ as $ L U x = b $.  Let $ U x = y $, which breaks the\n",
    "problem into two sub-problems.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L y &= b\\\\\n",
    "U x &= y\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "To demonstrate this, we can solve it by first using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Float64,1}:\n",
       "  0.49627820582486404\n",
       "  0.2520008871120075 \n",
       " -4.428091208429694  \n",
       "  5.119647290772903  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = L \\ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = U \\ y\n",
    "x ≈ A \\ b  # Check identical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LU decomposition also has specialized algorithms for structured matrices, such as a Tridiagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LU{Float64,Tridiagonal{Float64,Array{Float64,1}}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 1000\n",
    "b = rand(N)\n",
    "A = Tridiagonal([fill(0.1, N-2); 0.2], fill(0.8, N), [0.2; fill(0.1, N-2);])\n",
    "factorize(A) |> typeof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This factorization is the key to the performance of the `A \\ b` in this case.  For Tridiagonal matrices, the\n",
    "LU decomposition is $ O(N^2) $.\n",
    "\n",
    "Finally, just as a dense matrix without any structure will tend to use a LU decomposition to solve systems,\n",
    "so will a sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SuiteSparse.UMFPACK.UmfpackLU{Float64,Int64}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_sparse = sparse(A)\n",
    "factorize(A_sparse) |> typeof  # dropping the tridiagonal structure to just become sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\\b for typeof(A) = Tridiagonal{Float64,Array{Float64,1}}\n",
      "  52.100 μs (9 allocations: 47.75 KiB)\n",
      "A\\b for typeof(A) = SparseMatrixCSC{Float64,Int64}\n",
      "  537.199 μs (69 allocations: 1.06 MiB)\n"
     ]
    }
   ],
   "source": [
    "benchmark_solve(A, b)\n",
    "benchmark_solve(A_sparse, b);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With sparsity, the computational order is related to the number of non-zeros rather than the size of the matrix itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cholesky Decomposition\n",
    "\n",
    "For real, symmetric, positive definitive matrices, a Cholesky decomposition is a specialized version of the LU decomposition where $ L = U' $.\n",
    "\n",
    "The Cholesky is directly useful on its own (e.g. [Classical Control with Linear Algebra](time_series_models/classical_filtering.html)) but it is also an efficient factorization to solve symmetric positive definite system.\n",
    "\n",
    "As always, symmetry allows specialized algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BunchKaufman{Float64,Array{Float64,2}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 500\n",
    "B = rand(N,N)\n",
    "A_dense = B' * B  # an easy way to generate a symmetric positive definite matrix\n",
    "A = Symmetric(A_dense)\n",
    "\n",
    "factorize(A) |> typeof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the $ A $ decomposition is [Bunch-Kaufman](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/index.html#LinearAlgebra.bunchkaufman) rather than a\n",
    "Cholesky, because Julia doesn’t know the matrix is positive definite.  We can manually factorize with a cholesky,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cholesky{Float64,Array{Float64,2}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cholesky(A) |> typeof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmarking,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\\b for typeof(A) = Symmetric{Float64,Array{Float64,2}}\n",
      "  24.371 ms (8 allocations: 2.16 MiB)\n",
      "A\\b for typeof(A) = Array{Float64,2}\n",
      "  26.621 ms (5 allocations: 1.92 MiB)\n",
      "  4.256 ms (7 allocations: 1.91 MiB)\n"
     ]
    }
   ],
   "source": [
    "b = rand(N)\n",
    "cholesky(A) \\ b  # use the factorization to solve\n",
    "\n",
    "benchmark_solve(A, b)\n",
    "benchmark_solve(A_dense, b)\n",
    "@btime cholesky($A, check=false) \\ $b;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QR Decomposition\n",
    "\n",
    "[Previously](orth_proj.html#qr-decomposition) , we learned about applications of the QR application to solving the linear least squares.\n",
    "\n",
    "While in principle, the solution to least-squares is $ x = (A'A)^{-1}A'b $, in practice note that $ A'A $ becomes very dense and inverse are rarely a good idea.\n",
    "\n",
    "The QR decomposition is a decomposition $ A = Q R $ where $ Q $ is an orthogonal matrix (i.e. $ Q'Q = Q Q' = I $) and $ R $ is\n",
    "a upper triangular matrix.\n",
    "\n",
    "Given the  [previous derivation](orth_proj.html#qr-decomposition) we showed that the, given the decomposition, we can write the least squares problem as\n",
    "the solution to\n",
    "\n",
    "$$\n",
    "R x = Q' b\n",
    "$$\n",
    "\n",
    "Where, as discussed above, the upper-triangular structure of $ R $ can be solved easily with back substitution.\n",
    "\n",
    "For Julia, the `\\` operator will solve this problem whenever the given `A` is rectangular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       "  0.4818243357367359 \n",
       " -0.48449689054544554\n",
       "  0.18625388222391537"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 10\n",
    "M = 3\n",
    "x_true = rand(3)\n",
    "\n",
    "A = rand(N,M) .+ randn(N)\n",
    "b = rand(N)\n",
    "x = A \\ b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To manually use the QR decomposition: **Note** the real code would be more subtle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q * R ≈ A = true\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       "  0.4818243357367359 \n",
       " -0.48449689054544626\n",
       "  0.18625388222391615"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Af = qr(A)\n",
    "Q = Af.Q\n",
    "R = [Af.R; zeros(N - M, M)] # Stack with zeros\n",
    "@show Q * R ≈ A\n",
    "x = R \\ Q'*b  # the QR way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This stacks the `R` with zeros to multiple, but the more specialized algorithm would not multiply directly\n",
    "in that way.\n",
    "\n",
    "In some cases, if an LU is not available for a particular matrix structure, the QR factorization\n",
    "can also be used to solve systems of equations (i.e. not just LLS).  This tends to be about 2x slower than the LU,\n",
    "but is of the same computational order.\n",
    "\n",
    "Deriving the approach, where we can now use inverse since the system is square and we assumed $ A $ was non-singular\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "A x &= b\\\\\n",
    "Q R x &= b\\\\\n",
    "Q^{-1} Q R x &= Q^{-1} b\\\\\n",
    "R x &= Q' b\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Where the last step uses that $ Q^{-1} = Q' $ for orthogonal matrix.\n",
    "\n",
    "Given the decomposition, the solution for dense matrices is of computational\n",
    "order $ O(N^2) $.  To see this, look at the order of each operation.\n",
    "\n",
    "- Since $ R $ is upper-triangular matrix, it can be solved quickly through back substitution with computational order $ O(N^2) $  \n",
    "- A transpose operation is of order $ O(N^2) $  \n",
    "- A matrix-vector product is also $ O(N^2) $  \n",
    "\n",
    "\n",
    "In all cases, the order would drop depending on the sparsity pattern of the\n",
    "matrix (and corresponding decomposition).  A key benefit of a QR decomposition is that it tends to\n",
    "maintain sparsity.\n",
    "\n",
    "Without implementing the full process, you can form a QR\n",
    "factorization with `qr` and then use it to solve a system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A \\ b = [-0.2632496770433602, 1.1906001462220575, -1.4543207413238317, 2.1870422569978465, -0.8879236615218332]\n",
      "qr(A) \\ b = [-0.26324967704336, 1.1906001462220577, -1.454320741323831, 2.1870422569978447, -0.8879236615218327]\n"
     ]
    }
   ],
   "source": [
    "N = 5\n",
    "A = rand(N,N)\n",
    "b = rand(N)\n",
    "@show A \\ b\n",
    "@show qr(A) \\ b;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Decomposition\n",
    "\n",
    "A spectral decomposition, also known as an [eigendecomposition](https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix), finds all of the eigenvectors and eigenvalues to decompose a square matrix `A` such that\n",
    "\n",
    "$$\n",
    "A = Q \\Lambda Q^{-1}\n",
    "$$\n",
    "\n",
    "where $ Q $ is a matrix made of the the eigenvectors of $ A $ as columns, and $ \\Lambda $ is a diagonal matrix of the eigenvalues.  Only square, [diagonalizable](https://en.wikipedia.org/wiki/Diagonalizable_matrix) matrices have an eigendecomposition (where a matrix is not diagonalizable if it does not have a full set of linearly independent eigenvectors).\n",
    "\n",
    "In Julia, whenever you ask for a full set of eigenvectors and eigenvalues, it will find them through this decomposition using an algorithm appropriate for the matrix type.  For example, symmetric, hermitian, or tridiagonal matrices have their own algorithms.\n",
    "\n",
    "To see this in operation,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.804665742060837e-15"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = Symmetric(rand(5, 5))  # symmetric matrices have real eigenvectors/eigenvalues\n",
    "A_eig = eigen(A)\n",
    "Λ = Diagonal(A_eig.values)\n",
    "Q = A_eig.vectors\n",
    "norm(Q * Λ * inv(Q) - A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that, in general, a real matrix may have complex eigenvalues and eigenvectors, so if you attempt to multiple to check `Q * Λ * inv(Q) - A` which may not be exactly real due to numerical inaccuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Time Markov Chains (CTMC)\n",
    "\n",
    "In the previous lecture on discrete time Markov Chains, we saw that the transition probability\n",
    "between state $ x $ and state $ y $ was summarized by the matrix $ P(x, y) := \\mathbb P \\{ X_{t+1} = y \\,|\\, X_t = x \\} $.\n",
    "\n",
    "As a brief introduction to continuous time processes, consider same state-space as in the discrete\n",
    "case: $ S $ a finite set with $ n $ elements $ \\{x_1, \\ldots, x_n\\} $.\n",
    "\n",
    "A **Markov chain** $ \\{X_t\\} $ on $ S $ is a sequence of random variables on $ S $ that have the **Markov property**\n",
    "\n",
    "In continuous time, the [Markov Property](https://en.wikipedia.org/wiki/Markov_property) is more complicated, but intuitively is\n",
    "the same as the discrete time case.\n",
    "\n",
    "That is, knowing the current state is enough to know probabilities for future states.  Or, for realizations $ x(\\tau)\\in S, \\tau \\leq t $,\n",
    "\n",
    "$$\n",
    "\\mathbb P \\{ X(t+s) = y  \\,|\\, X(t) = x, X(\\tau) = x(\\tau) \\text{ for } 0 \\leq \\tau \\leq t  \\} = \\mathbb P \\{ X(t+s) = y  \\,|\\, X(t) = x\\}\n",
    "$$\n",
    "\n",
    "Heuristically, consider a time period $ t $ and a small step forward $ \\Delta $.  Then the probability to transition from state $ i $ to\n",
    "state $ j $ is\n",
    "\n",
    "$$\n",
    "\\mathbb P \\{ X(t + \\Delta) = j  \\,|\\, X(t) \\} = \\begin{cases} q_{ij} \\Delta + o(\\Delta) & i \\neq j\\\\\n",
    "                                                              1 + q_{ii} \\Delta + o(\\Delta) & i = j \\end{cases}\n",
    "$$\n",
    "\n",
    "where $ q_{ij} $ are parameters governing the transition process, and $ o(\\Delta) $ is [little-o notation](https://en.wikipedia.org/wiki/Big_O_notation#Little-o_notation),.  That is, $ \\lim_{\\Delta\\to 0} o(\\Delta)/\\Delta = 0 $.\n",
    "\n",
    "Just as in the discrete case, we can summarize these parameters by a $ N \\times N $ matrix, $ Q \\in R^{N\\times N} $.\n",
    "\n",
    "The $ Q $ matrix is called the intensity matrix, or the infinitesimal generator of the Markov Chain.  For example,\n",
    "\n",
    "$$\n",
    "Q = \\begin{bmatrix} -0.1 & 0.1  & 0 & 0 & 0 & 0\\\\\n",
    "                    0.1  &-0.2  & 0.1 &  0 & 0 & 0\\\\\n",
    "                    0 & 0.1 & -0.2 & 0.1 & 0 & 0\\\\\n",
    "                    0 & 0 & 0.1 & -0.2 & 0.1 & 0\\\\\n",
    "                    0 & 0 & 0 & 0.1 & -0.2 & 0.1\\\\\n",
    "                    0 & 0 & 0 & 0 & 0.1 & -0.1\\\\\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In that example, transitions only occur between adjacent states with the same intensity (except for a ``bouncing’’ back of the bottom and top states)\n",
    "\n",
    "This also demonstrates that the elements of the intensity matrix are not probabilities.  Unlike the discrete case, where every row must sum to one, the rows of $ Q $ sum to zero, where the diagonal contains the negative value of jumping out of the current state.  That is\n",
    "\n",
    "- $ q_{ij} \\geq 0 $ for $ i \\neq j $  \n",
    "- $ q_{ii} \\leq 0 $  \n",
    "- $ \\sum_{j} q_{ij} = 0 $  \n",
    "\n",
    "\n",
    "Implementing $ Q $ using its tridiagonal structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6×6 Tridiagonal{Float64,Array{Float64,1}}:\n",
       " -0.1   0.1    ⋅     ⋅     ⋅     ⋅ \n",
       "  0.1  -0.2   0.1    ⋅     ⋅     ⋅ \n",
       "   ⋅    0.1  -0.2   0.1    ⋅     ⋅ \n",
       "   ⋅     ⋅    0.1  -0.2   0.1    ⋅ \n",
       "   ⋅     ⋅     ⋅    0.1  -0.2   0.1\n",
       "   ⋅     ⋅     ⋅     ⋅    0.1  -0.1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "α = 0.1\n",
    "N = 6\n",
    "Q = Tridiagonal(fill(α, N-1), [-α; fill(-2α, N-2); -α], fill(α, N-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use a Tridiagonal to exploit the structure of the problem.\n",
    "\n",
    "Consider a simple payoff vector $ p $ associated with each state, and a discount rate $ ρ $.  Then we can solve for\n",
    "the expected present discounted value in a similar way to the discrete time case.\n",
    "\n",
    "$$\n",
    "\\rho v = p + Q v\n",
    "$$\n",
    "\n",
    "or rearranging slightly, solving the linear system\n",
    "\n",
    "$$\n",
    "(\\rho I - Q) v = p\n",
    "$$\n",
    "\n",
    "For our example, exploiting the tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6×6 Tridiagonal{Float64,Array{Float64,1}}:\n",
       "  0.15  -0.1     ⋅      ⋅      ⋅      ⋅  \n",
       " -0.1    0.25  -0.1     ⋅      ⋅      ⋅  \n",
       "   ⋅    -0.1    0.25  -0.1     ⋅      ⋅  \n",
       "   ⋅      ⋅    -0.1    0.25  -0.1     ⋅  \n",
       "   ⋅      ⋅      ⋅    -0.1    0.25  -0.1 \n",
       "   ⋅      ⋅      ⋅      ⋅    -0.1    0.15"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = range(0.0, 10.0, length=N)\n",
    "ρ = 0.05\n",
    "\n",
    "A = ρ * I - Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this $ A $ matrix is maintaining the tridiagonal structure of the problem, which leads to an efficient solution to the\n",
    "linear problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6-element Array{Float64,1}:\n",
       "  38.15384615384615\n",
       "  57.23076923076923\n",
       "  84.92307692307693\n",
       " 115.07692307692311\n",
       " 142.76923076923077\n",
       " 161.84615384615384"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = A \\ p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $ Q $ is also used to calculate the evolution of the Markov chain, in direct analogy to the $ ψ_{t+k} = ψ_t P^k $ evolution with transition matrix $ P $ of the discrete case.\n",
    "\n",
    "In the continuous case, this becomes the system of linear differential equations\n",
    "\n",
    "$$\n",
    "\\dot{ψ}(t) = Q(t)^T ψ(t)\n",
    "$$\n",
    "\n",
    "given the initial condition $ ψ(0) $ and where the $ Q(t) $ intensity matrix is allows to vary with time.  In the simplest case of a constant $ Q $ matrix, this is a simple constant-coefficient system of Linear ODEs with coefficients $ Q^T $\n",
    "\n",
    "If a stationary equilibria exists, note that $ \\dot{ψ}(t) = 0 $, and the stationary solution $ ψ^{*} $ would need to fulfill\n",
    "\n",
    "$$\n",
    "0 = Q^T ψ^{*}\n",
    "$$\n",
    "\n",
    "Notice that this is of the form $ 0 ψ^{*} = Q^T ψ^{*} $ and hence is equivalent to finding the eigevector associated with the $ \\lambda = 0 $ eigenvalue\n",
    "\n",
    "With our example, we can calculate all of the eigenvalues and eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Eigen{Float64,Float64,Array{Float64,2},Array{Float64,1}}\n",
       "eigenvalues:\n",
       "6-element Array{Float64,1}:\n",
       " -0.3732050807568874  \n",
       " -0.29999999999999993 \n",
       " -0.19999999999999998 \n",
       " -0.09999999999999995 \n",
       " -0.026794919243112274\n",
       "  0.0                 \n",
       "eigenvectors:\n",
       "6×6 Array{Float64,2}:\n",
       " -0.149429  -0.288675   0.408248   0.5          -0.557678  0.408248\n",
       "  0.408248   0.57735   -0.408248   1.38778e-16  -0.408248  0.408248\n",
       " -0.557678  -0.288675  -0.408248  -0.5          -0.149429  0.408248\n",
       "  0.557678  -0.288675   0.408248  -0.5           0.149429  0.408248\n",
       " -0.408248   0.57735    0.408248   7.63278e-16   0.408248  0.408248\n",
       "  0.149429  -0.288675  -0.408248   0.5           0.557678  0.408248"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "λ, vecs = eigen(Array(Q'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, there is a $ \\lambda = 0 $ eigenvalue, which is associated with the last column in the eigenvector.  To turn that into a probability\n",
    "we need to normalize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6-element Array{Float64,1}:\n",
       " 0.16666666666666657\n",
       " 0.16666666666666657\n",
       " 0.1666666666666667 \n",
       " 0.16666666666666682\n",
       " 0.16666666666666685\n",
       " 0.16666666666666663"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs[:,N] ./ sum(vecs[:,N])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Dimensions\n",
    "\n",
    "A frequent case in discretized models is dealing with Markov chains with multiple “spatial” dimensions (e.g. wealth and income).\n",
    "\n",
    "After discretizing a process to create a Markov chain, you can always take the cartesian product of the set of states in order to\n",
    "enumerate as a single finite state.\n",
    "\n",
    "To see this, consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8×8 SparseMatrixCSC{Float64,Int64} with 28 stored entries:\n",
       "  [1, 1]  =  -0.2\n",
       "  [2, 1]  =  0.1\n",
       "  [5, 1]  =  0.2\n",
       "  [1, 2]  =  0.1\n",
       "  [2, 2]  =  -0.3\n",
       "  [3, 2]  =  0.1\n",
       "  [6, 2]  =  0.2\n",
       "  [2, 3]  =  0.1\n",
       "  [3, 3]  =  -0.3\n",
       "  [4, 3]  =  0.1\n",
       "  [7, 3]  =  0.2\n",
       "  [3, 4]  =  0.1\n",
       "  ⋮\n",
       "  [5, 5]  =  -0.3\n",
       "  [6, 5]  =  0.1\n",
       "  [2, 6]  =  0.1\n",
       "  [5, 6]  =  0.1\n",
       "  [6, 6]  =  -0.4\n",
       "  [7, 6]  =  0.1\n",
       "  [3, 7]  =  0.1\n",
       "  [6, 7]  =  0.1\n",
       "  [7, 7]  =  -0.4\n",
       "  [8, 7]  =  0.1\n",
       "  [4, 8]  =  0.1\n",
       "  [7, 8]  =  0.1\n",
       "  [8, 8]  =  -0.3"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using SparseArrays\n",
    "function markov_chain_product(Q, A)\n",
    "    M = size(Q, 1)\n",
    "    N = size(A, 1)\n",
    "    Q = sparse(Q)\n",
    "    Qs = blockdiag(fill(Q, N)...)  # create diagonal blocks of every operator\n",
    "    As = kron(A, sparse(I(M)))\n",
    "    return As + Qs\n",
    "end\n",
    "\n",
    "α = 0.1\n",
    "N = 4\n",
    "Q = Tridiagonal(fill(α, N-1), [-α; fill(-2α, N-2); -α], fill(α, N-1))\n",
    "A = sparse([-0.1 0.1\n",
    "    0.2 -0.2])\n",
    "M = size(A,1)\n",
    "L = markov_chain_product(Q, A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the sparsity pattern,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip5700\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polygon clip-path=\"url(#clip5700)\" points=\"\n",
       "0,1600 2400,1600 2400,0 0,0 \n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip5701\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polygon clip-path=\"url(#clip5700)\" points=\"\n",
       "393.14,1487.47 1833.37,1487.47 1833.37,47.2441 393.14,47.2441 \n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip5702\">\n",
       "    <rect x=\"393\" y=\"47\" width=\"1441\" height=\"1441\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip5700)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  393.14,1487.47 1833.37,1487.47 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5700)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  393.14,47.2441 393.14,1487.47 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5700)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  393.14,1487.47 393.14,1465.87 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5700)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  598.887,1487.47 598.887,1465.87 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5700)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  804.634,1487.47 804.634,1465.87 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5700)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1010.38,1487.47 1010.38,1465.87 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5700)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1216.13,1487.47 1216.13,1465.87 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5700)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1421.88,1487.47 1421.88,1465.87 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5700)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1627.62,1487.47 1627.62,1465.87 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5700)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1833.37,1487.47 1833.37,1465.87 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5700)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  393.14,47.2441 414.743,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5700)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  393.14,252.991 414.743,252.991 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5700)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  393.14,458.739 414.743,458.739 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5700)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  393.14,664.486 414.743,664.486 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5700)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  393.14,870.233 414.743,870.233 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5700)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  393.14,1075.98 414.743,1075.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5700)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  393.14,1281.73 414.743,1281.73 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5700)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  393.14,1487.47 414.743,1487.47 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip5700)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 393.14, 1541.47)\" x=\"393.14\" y=\"1541.47\">1</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5700)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 598.887, 1541.47)\" x=\"598.887\" y=\"1541.47\">2</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5700)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 804.634, 1541.47)\" x=\"804.634\" y=\"1541.47\">3</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5700)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1010.38, 1541.47)\" x=\"1010.38\" y=\"1541.47\">4</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5700)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1216.13, 1541.47)\" x=\"1216.13\" y=\"1541.47\">5</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5700)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1421.88, 1541.47)\" x=\"1421.88\" y=\"1541.47\">6</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5700)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1627.62, 1541.47)\" x=\"1627.62\" y=\"1541.47\">7</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5700)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1833.37, 1541.47)\" x=\"1833.37\" y=\"1541.47\">8</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5700)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 369.14, 64.7441)\" x=\"369.14\" y=\"64.7441\">1</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5700)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 369.14, 270.491)\" x=\"369.14\" y=\"270.491\">2</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5700)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 369.14, 476.239)\" x=\"369.14\" y=\"476.239\">3</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5700)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 369.14, 681.986)\" x=\"369.14\" y=\"681.986\">4</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5700)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 369.14, 887.733)\" x=\"369.14\" y=\"887.733\">5</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5700)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 369.14, 1093.48)\" x=\"369.14\" y=\"1093.48\">6</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5700)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 369.14, 1299.23)\" x=\"369.14\" y=\"1299.23\">7</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5700)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 369.14, 1504.97)\" x=\"369.14\" y=\"1504.97\">8</text>\n",
       "</g>\n",
       "<circle clip-path=\"url(#clip5702)\" style=\"fill:#781c6d; stroke:none; fill-opacity:1\" cx=\"393.14\" cy=\"47.2441\" r=\"36\"/>\n",
       "<circle clip-path=\"url(#clip5702)\" style=\"fill:#fbb419; stroke:none; fill-opacity:1\" cx=\"393.14\" cy=\"252.991\" r=\"36\"/>\n",
       "<circle clip-path=\"url(#clip5702)\" style=\"fill:#fcfea4; stroke:none; fill-opacity:1\" cx=\"393.14\" cy=\"870.233\" r=\"36\"/>\n",
       "<circle clip-path=\"url(#clip5702)\" style=\"fill:#fbb419; stroke:none; fill-opacity:1\" cx=\"598.887\" cy=\"47.2441\" r=\"36\"/>\n",
       "<circle clip-path=\"url(#clip5702)\" style=\"fill:#33095d; stroke:none; fill-opacity:1\" cx=\"598.887\" cy=\"252.991\" r=\"36\"/>\n",
       "<circle clip-path=\"url(#clip5702)\" style=\"fill:#fbb419; stroke:none; fill-opacity:1\" cx=\"598.887\" cy=\"458.739\" r=\"36\"/>\n",
       "<circle clip-path=\"url(#clip5702)\" style=\"fill:#fcfea4; stroke:none; fill-opacity:1\" cx=\"598.887\" cy=\"1075.98\" r=\"36\"/>\n",
       "<circle clip-path=\"url(#clip5702)\" style=\"fill:#fbb419; stroke:none; fill-opacity:1\" cx=\"804.634\" cy=\"252.991\" r=\"36\"/>\n",
       "<circle clip-path=\"url(#clip5702)\" style=\"fill:#33095d; stroke:none; fill-opacity:1\" cx=\"804.634\" cy=\"458.739\" r=\"36\"/>\n",
       "<circle clip-path=\"url(#clip5702)\" style=\"fill:#fbb419; stroke:none; fill-opacity:1\" cx=\"804.634\" cy=\"664.486\" r=\"36\"/>\n",
       "<circle clip-path=\"url(#clip5702)\" style=\"fill:#fcfea4; stroke:none; fill-opacity:1\" cx=\"804.634\" cy=\"1281.73\" r=\"36\"/>\n",
       "<circle clip-path=\"url(#clip5702)\" style=\"fill:#fbb419; stroke:none; fill-opacity:1\" cx=\"1010.38\" cy=\"458.739\" r=\"36\"/>\n",
       "<circle clip-path=\"url(#clip5702)\" style=\"fill:#781c6d; stroke:none; fill-opacity:1\" cx=\"1010.38\" cy=\"664.486\" r=\"36\"/>\n",
       "<circle clip-path=\"url(#clip5702)\" style=\"fill:#fcfea4; stroke:none; fill-opacity:1\" cx=\"1010.38\" cy=\"1487.47\" r=\"36\"/>\n",
       "<circle clip-path=\"url(#clip5702)\" style=\"fill:#fbb419; stroke:none; fill-opacity:1\" cx=\"1216.13\" cy=\"47.2441\" r=\"36\"/>\n",
       "<circle clip-path=\"url(#clip5702)\" style=\"fill:#33095d; stroke:none; fill-opacity:1\" cx=\"1216.13\" cy=\"870.233\" r=\"36\"/>\n",
       "<circle clip-path=\"url(#clip5702)\" style=\"fill:#fbb419; stroke:none; fill-opacity:1\" cx=\"1216.13\" cy=\"1075.98\" r=\"36\"/>\n",
       "<circle clip-path=\"url(#clip5702)\" style=\"fill:#fbb419; stroke:none; fill-opacity:1\" cx=\"1421.88\" cy=\"252.991\" r=\"36\"/>\n",
       "<circle clip-path=\"url(#clip5702)\" style=\"fill:#fbb419; stroke:none; fill-opacity:1\" cx=\"1421.88\" cy=\"870.233\" r=\"36\"/>\n",
       "<circle clip-path=\"url(#clip5702)\" style=\"fill:#000003; stroke:none; fill-opacity:1\" cx=\"1421.88\" cy=\"1075.98\" r=\"36\"/>\n",
       "<circle clip-path=\"url(#clip5702)\" style=\"fill:#fbb419; stroke:none; fill-opacity:1\" cx=\"1421.88\" cy=\"1281.73\" r=\"36\"/>\n",
       "<circle clip-path=\"url(#clip5702)\" style=\"fill:#fbb419; stroke:none; fill-opacity:1\" cx=\"1627.62\" cy=\"458.739\" r=\"36\"/>\n",
       "<circle clip-path=\"url(#clip5702)\" style=\"fill:#fbb419; stroke:none; fill-opacity:1\" cx=\"1627.62\" cy=\"1075.98\" r=\"36\"/>\n",
       "<circle clip-path=\"url(#clip5702)\" style=\"fill:#000003; stroke:none; fill-opacity:1\" cx=\"1627.62\" cy=\"1281.73\" r=\"36\"/>\n",
       "<circle clip-path=\"url(#clip5702)\" style=\"fill:#fbb419; stroke:none; fill-opacity:1\" cx=\"1627.62\" cy=\"1487.47\" r=\"36\"/>\n",
       "<circle clip-path=\"url(#clip5702)\" style=\"fill:#fbb419; stroke:none; fill-opacity:1\" cx=\"1833.37\" cy=\"664.486\" r=\"36\"/>\n",
       "<circle clip-path=\"url(#clip5702)\" style=\"fill:#fbb419; stroke:none; fill-opacity:1\" cx=\"1833.37\" cy=\"1281.73\" r=\"36\"/>\n",
       "<circle clip-path=\"url(#clip5702)\" style=\"fill:#33095d; stroke:none; fill-opacity:1\" cx=\"1833.37\" cy=\"1487.47\" r=\"36\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip5703\">\n",
       "    <rect x=\"1881\" y=\"47\" width=\"73\" height=\"1441\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<g clip-path=\"url(#clip5703)\">\n",
       "<image width=\"72\" height=\"1440\" xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAEgAAAWgCAYAAAD9wgUPAAAMT0lEQVR4nO3dwY3sRhAFQY5Q/lsh\n",
       "L6XfLQtUeSQPERYsEg8kmpyZ/f17/r4P/+uvt/+ArxMoCBQECgKFOffft/+GT7OgIFAQKAgUBApz\n",
       "3cVWFhQECgIFgcLc4yK9saAgUBAoCBQECo4awYKCQEGgIFAQKLiLBQsKAgWBgkDBRTpYUBAoCBQE\n",
       "CgIFr32CBQWBgkBBoOCoESwoCBQECgIFgYK7WLCgIFAQKAgUBArzeGC2sqAgUBAoCBRcpIMFBYGC\n",
       "QEGgIFCYxwOzlQUFgYJAQaAgUHAWCxYUBAoCBYGCi3SwoCBQECgIFAQK85x/3v4bPs2CgkBBoCBQ\n",
       "8FYjWFAQKAgUBAoCBQ/MggUFgYJAQaAgUHAXCxYUBAoCBYHC/O6ft/+GT7OgIFAQKAgUBArzHHex\n",
       "jQUFgYJAQaAgUJifu9jKgoJAQaAgUHDUCBYUBAoCBYGCQGEer31WFhQECgIFgYLnQcGCgkBBoCBQ\n",
       "ECh4YBYsKAgUBAoCBYGCs1iwoCBQECgIFBw1ggUFgYJAQaAgUJjfOW//DZ9mQUGgIFAQKAgUnMWC\n",
       "BQWBgkBBoOAiHSwoCBQECgIFgYJ388GCgkBBoCBQmMdbjZUFBYGCQEGgIFBw1AgWFAQKAgWBgkDB\n",
       "a59gQUGgIFAQKLhIBwsKAgWBgkBBoODLLMGCgkBBoCBQECj48EKwoCBQECgIFDwwCxYUBAoCBYGC\n",
       "QMFRI1hQECgIFAQKLtLBgoJAQaAgUBAoeDcfLCgIFAQKAgWBgtc+wYKCQEGgIFDwwCxYUBAoCBQE\n",
       "CgIFd7FgQUGgIFAQKAgU3MWCBQWBgkBBoDDPuW//DZ9mQUGgIFAQKAgU5rmOGhsLCgIFgYJAwfOg\n",
       "YEFBoCBQECgIFDwwCxYUBAoCBYGCQMFZLFhQECgIFAQKLtLBgoJAQaAgUBAoeGAWLCgIFAQKAgWB\n",
       "grNYsKAgUBAoCBTmcY1eWVAQKAgUBAoCBXexYEFBoCBQECiMLxzuLCgIFAQKAgWBgqNGsKAgUBAo\n",
       "CBQECu5iwYKCQEGgIFBwkQ4WFAQKAgWBgkBhHt9lWVlQECgIFAQKAoW55/f23/BpFhQECgIFgYIH\n",
       "ZsGCgkBBoCBQECjM46ixsqAgUBAoCBTmXhfpjQUFgYJAQaAgUHDUCBYUBAoCBYGCQMFdLFhQECgI\n",
       "FAQKHpgFCwoCBYGCQEGg4KgRLCgIFAQKAgWBwjxHo406QaAgUBAoeGAWLCgIFAQKAgWBggdmwYKC\n",
       "QEGgIFDw4ybBgoJAQaAgUBAozHM12qgTBAoCBYGCQMFZLFhQECgIFAQKLtLBgoJAQaAgUBAo+Bhw\n",
       "UCcIFAQKAgWBgs9JBwsKAgWBgkDBx4CDBQWBgkBBoCBQmOuB2UqdIFAQKAgUPA8KFhQECgIFgYJA\n",
       "wQOzYEFBoCBQECgIFDwwC+oEgYJAQaDggVmwoCBQECgIFAQKvpIZLCgIFAQKAgWBwly/YbZSJwgU\n",
       "BAoCBe/mgwUFgYJAQaAgUPDALFhQECgIFAQK3s0HCwoCBYGCQEGg4GPAQZ0gUBAoCBQECs5iwYKC\n",
       "QEGgIFDwViNYUBAoCBQECgIFR41gQUGgIFAQKAgUvPYJ6gSBgkBBoOCoESwoCBQECgIFgYLXPsGC\n",
       "gkBBoCBQcNQIFhQECgIFgYJAwVuNoE4QKAgUBAoCBWexYEFBoCBQECi4SAcLCgIFgYJAQaDgLhYs\n",
       "KAgUBAoCBYGCT5gFCwoCBYGCQGGOf5+1UicIFAQKAgWBggdmwYKCQEGgIFDwPChYUBAoCBQECgIF\n",
       "R41gQUGgIFAQKAgU3MWCBQWBgkBBoOAiHSwoCBQECgIFgYK7WLCgIFAQKAgUBApz3MVWFhQECgIF\n",
       "gYKjRrCgIFAQKAgUBAruYsGCgkBBoCBQcJEOFhQECgIFgYJAwV0sWFAQKAgUBAoCBXexYEFBoCBQ\n",
       "ECj4AFWwoCBQECgIFAQKjhrBgoJAQaAgUBAouIsFCwoCBYGCQMF/ZgnqBIGCQEGgIFBw1AgWFAQK\n",
       "AgWBgnfzwYKCQEGgIFAQKDhqBAsKAgWBgkBBoOAsFiwoCBQECgIFR41gQUGgIFAQKAgU3MWCBQWB\n",
       "gkBBoCBQ8MAsWFAQKAgUBApzHxfpjQUFgYJAQaAgUHDUCBYUBAoCBYGCtxrBgoJAQaAgUBAozLlv\n",
       "/wnfZkFBoCBQECgIFJzFggUFgYJAQaDgrUawoCBQECgIFAQKc3zCbGVBQaAgUBAoCBQ8MAsWFAQK\n",
       "AgWBggdmwYKCQEGgIFAQKDhqBAsKAgWBgkBhztt/wcdZUBAoCBQECgIFR41gQUGgIFAQKAgUvPYJ\n",
       "FhQECgIFgYIfmgwWFAQKAgWBgkDBUSNYUBAoCBQECgIFv2EWLCgIFAQKAgUPzIIFBYGCQEGgIFDw\n",
       "CbNgQUGgIFAQKPgyS7CgIFAQKAgUBAqOGsGCgkBBoCBQECg4iwULCgIFgYJAwVEjWFAQKAgUBAoC\n",
       "BV9mCRYUBAoCBYGCQGF8l2VnQUGgIFAQKDhqBAsKAgWBgkBBoOCoESwoCBQECgIFR41gQUGgIFAQ\n",
       "KAgUHDWCBQWBgkBBoCBQcBYLFhQECgIFgYJfAw4WFAQKAgWBgkDBzyUHCwoCBYGCQEGg4CwWLCgI\n",
       "FAQKAoU5jhorCwoCBYGCQEGgMNdRY2VBQaAgUBAo+ABVsKAgUBAoCBQECr7MEiwoCBQECgIFgYJ3\n",
       "88GCgkBBoCBQ8G4+WFAQKAgUBAoCBe/mgwUFgYJAQaAgUPCvjIMFBYGCQEGg4BNmwYKCQEGgIFAQ\n",
       "KHhgFiwoCBQECgIFz4OCBQWBgkBBoCBQcNQIFhQECgIFgYJAweekgwUFgYJAQaDgG4fBgoJAQaAg\n",
       "UBAo+PWXYEFBoCBQECgIFJzFggUFgYJAQaDgIh0sKAgUBAoCBYGCf2UcLCgIFAQKAgVHjWBBQaAg\n",
       "UBAoCBS8mw8WFAQKAgWBgkDBWSxYUBAoCBQECi7SwYKCQEGgIFAQKHhgFiwoCBQECgIFgYKzWLCg\n",
       "IFAQKAgU/FxysKAgUBAoCBQECn4uOVhQECgIFAQKngcFCwoCBYGCQEGg4N18sKAgUBAoCBQECs5i\n",
       "wYKCQEGgIFCY+7hKbywoCBQECgIFgYKjRrCgIFAQKAgUBApe+wQLCgIFgYJAwVEjWFAQKAgUBAoC\n",
       "BUeNYEFBoCBQECg4agQLCgIFgYJAQaDge/PBgoJAQaAgUBAozPE56ZUFBYGCQEGg4KgRLCgIFAQK\n",
       "AgWBgqNGsKAgUBAoCBQECn70P1hQECgIFAQKcz0xW1lQECgIFAQKAgVHjWBBQaAgUBAo+DJLsKAg\n",
       "UBAoCBQECn4uOVhQECgIFAQKAgVnsWBBQaAgUBAo+BhwsKAgUBAoCBQECh6YBQsKAgWBgkBBoOAs\n",
       "FiwoCBQECgIFF+lgQUGgIFAQKAgU5vhK5sqCgkBBoCBQ8FYjWFAQKAgUBAoCBQ/MggUFgYJAQaAg\n",
       "UHAXCxYUBAoCBYGCi3SwoCBQECgIFAQKcx8/NbmxoCBQECgIFAQKzmLBgoJAQaAgUJjzc9TYWFAQ\n",
       "KAgUBAoChTkemK0sKAgUBAoCBRfpYEFBoCBQECgIFLybDxYUBAoCBYGCQGHOz7v5jQUFgYJAQaDg\n",
       "gVmwoCBQECgIFAQK7mLBgoJAQaAgUBAoeO0TLCgIFAQKAoU5z5+3/4ZPs6AgUBAoCBQECh6YBQsK\n",
       "AgWBgkBhrqPGyoKCQEGgIFAQKDhqBAsKAgWBgkBBoOAuFiwoCBQECgIFD8yCBQWBgkBBoCBQcNQI\n",
       "FhQECgIFgYJAwVksWFAQKAgUBAq+cRgsKAgUBAoCBYHCnOsutrGgIFAQKAgUHDWCBQWBgkBBoCBQ\n",
       "mOuosbKgIFAQKAgUBArOYsGCgkBBoCBQ8AGqYEFBoCBQECgIFLybDxYUBAoCBYGCQMEDs2BBQaAg\n",
       "UBAoeDcfLCgIFAQKAgWBgtc+wYKCQEGgIFBw1AgWFAQKAgWBgkDBW41gQUGgIFAQKAgUnMWCBQWB\n",
       "gkBBoOCtRrCgIFAQKAgUBAqOGsGCgkBBoCBQECh47RMsKAgUBAoChbnXA7ONBQWBgkBBoCBQcNQI\n",
       "FhQECgIFgcI89779N3yaBQWBgkBBoCBQmMdRY2VBQaAgUBAoCBR8wixYUBAoCBQECo4awYKCQEGg\n",
       "IFAQKHg3HywoCBQECgIFgcI8HpitLCgIFAQKAgVHjWBBQaAgUBAoCBS89gkWFAQKAgWBwn8v2/BP\n",
       "X8iooQAAAABJRU5ErkJggg==\n",
       "\" transform=\"translate(1881, 47)\"/>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5700)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 1989.37, 1501.13)\" x=\"1989.37\" y=\"1501.13\">-</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5700)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 2017.48, 1501.13)\" x=\"2017.48\" y=\"1501.13\">0.4</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5700)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 1989.37, 1261.09)\" x=\"1989.37\" y=\"1261.09\">-</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5700)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 2017.48, 1261.09)\" x=\"2017.48\" y=\"1261.09\">0.3</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5700)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 1989.37, 1021.05)\" x=\"1989.37\" y=\"1021.05\">-</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5700)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 2017.48, 1021.05)\" x=\"2017.48\" y=\"1021.05\">0.2</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5700)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 1989.37, 781.011)\" x=\"1989.37\" y=\"781.011\">-</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5700)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 2017.48, 781.011)\" x=\"2017.48\" y=\"781.011\">0.1</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5700)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 1989.37, 540.972)\" x=\"1989.37\" y=\"540.972\">0</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5700)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 1989.37, 300.934)\" x=\"1989.37\" y=\"300.934\">0.1</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5700)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 1989.37, 60.8953)\" x=\"1989.37\" y=\"60.8953\">0.2</text>\n",
       "</g>\n",
       "<polyline clip-path=\"url(#clip5700)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1953.37,1487.47 1953.37,1487.47 1977.37,1487.47 1953.37,1487.47 1953.37,1247.44 1977.37,1247.44 1953.37,1247.44 1953.37,1007.4 1977.37,1007.4 1953.37,1007.4 \n",
       "  1953.37,767.359 1977.37,767.359 1953.37,767.359 1953.37,527.321 1977.37,527.321 1953.37,527.321 1953.37,287.283 1977.37,287.283 1953.37,287.283 1953.37,47.2441 \n",
       "  1977.37,47.2441 1953.37,47.2441 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip5700)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(-90, 2130.97, 767.359)\" x=\"2130.97\" y=\"767.359\"></text>\n",
       "</g>\n",
       "</svg>\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Plots\n",
    "spy(L, markersize = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate a simple dynamic pricing problem, consider if the payoff of being in state $ (i,j) $ is $ r_{ij} = i + 2j $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8-element Array{Float64,1}:\n",
       " 3.0\n",
       " 4.0\n",
       " 5.0\n",
       " 6.0\n",
       " 5.0\n",
       " 6.0\n",
       " 7.0\n",
       " 8.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = [i + 2.0j for i in 1:N, j in 1:M]\n",
    "r = vec(r)  # vectorize it since stacked in same order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving the equation $ \\rho v = r + L v $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×2 Array{Float64,2}:\n",
       "  87.8992   93.6134\n",
       "  96.1345  101.849 \n",
       " 106.723   112.437 \n",
       " 114.958   120.672 "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ρ = 0.05\n",
    "v = (ρ * I - L) \\ r\n",
    "reshape(v, N, M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the stationary distribution, we find the eigenvalue and choose the eigenvector associated with $ \\lambda=0 $ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8-element Array{Float64,1}:\n",
       " 0.16666666666666669\n",
       " 0.1666666666666665 \n",
       " 0.16666666666666669\n",
       " 0.1666666666666668 \n",
       " 0.08333333333333327\n",
       " 0.08333333333333344\n",
       " 0.08333333333333331\n",
       " 0.0833333333333334 "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_eig = eigen(Matrix(L'))\n",
    "@assert norm(L_eig.values[end]) < 1E-10\n",
    "\n",
    "ϕ = L_eig.vectors[:,end]\n",
    "ϕ = ϕ / sum(ϕ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reshape this to be two dimensional if it is helpful for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×2 Array{Float64,2}:\n",
       " 0.166667  0.0833333\n",
       " 0.166667  0.0833333\n",
       " 0.166667  0.0833333\n",
       " 0.166667  0.0833333"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshape(ϕ, N, size(A,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Irreducibility\n",
    "\n",
    "As with the discrete time Markov chains, a key question is whether CTMCs are reducible, i.e. states communicate.  The problem\n",
    "is isomorphic to determining if the directed graph of the Markov chain is [strongly connected](https://en.wikipedia.org/wiki/Strongly_connected_component)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Recompiling stale cache file C:\\Users\\jlperla\\.julia\\compiled\\v1.2\\LightGraphs\\Xm08G.ji for LightGraphs [093fc24a-ae57-5d10-9952-331d41423f4d]\n",
      "└ @ Base loading.jl:1240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_strongly_connected(Q_graph) = true\n"
     ]
    }
   ],
   "source": [
    "using LinearAlgebra, LightGraphs\n",
    "α = 0.1\n",
    "N = 6\n",
    "Q = Tridiagonal(fill(α, N-1), [-α; fill(-2α, N-2); -α], fill(α, N-1))\n",
    "Q_graph = DiGraph(Q)\n",
    "@show is_strongly_connected(Q_graph);  # i.e. can follow directional edges to get to every state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or as an example of a reducible Markov chain,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_strongly_connected(Q_graph) = false\n"
     ]
    }
   ],
   "source": [
    "Q = [-0.2 0.2 0\n",
    "    0.2 -0.2 0\n",
    "    0.2 0.6 -0.2]\n",
    "Q_graph = DiGraph(Q)\n",
    "@show is_strongly_connected(Q_graph);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Banded Matrices\n",
    "\n",
    "A tridiagonal matrix has 3 non-zero diagonals.  The main diagonal, the first sub-diagonal (i.e. below the main diagonal) and the also the first super-diagonal (i.e. above the main diagonal).\n",
    "\n",
    "This is a special case of a more general type called a banded matrix, where the number of sub and super-diagonals are more general.  The\n",
    "total width of sub- and super-diagonals is called the bandwidth.  For example, a tridiagonal matrix has a bandwidth of 3.\n",
    "\n",
    "A $ N \\times N $ banded matrix with bandwidth $ P $ has about $ N P $ nonzeros in its sparsity pattern.\n",
    "\n",
    "These can be created directly as a dense matrix with `diagm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×4 Array{Int64,2}:\n",
       " 0  1  0  0\n",
       " 4  0  2  0\n",
       " 0  5  0  3\n",
       " 0  0  0  0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diagm(1 => [1,2,3], -1 => [4,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or as a sparse matrix,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×4 SparseMatrixCSC{Int64,Int64} with 5 stored entries:\n",
       "  [2, 1]  =  4\n",
       "  [1, 2]  =  1\n",
       "  [3, 2]  =  5\n",
       "  [2, 3]  =  2\n",
       "  [3, 4]  =  3"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spdiagm(1 => [1,2,3], -1 => [4,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a simple banded matrix, using [BandedMatrices.jl](https://github.com/JuliaMatrices/BandedMatrices.jl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6×6 BandedMatrix{Int64,Array{Int64,2},Base.OneTo{Int64}}:\n",
       " 0  0  1  ⋅  ⋅  ⋅\n",
       " 1  0  0  2  ⋅  ⋅\n",
       " ⋅  2  0  0  3  ⋅\n",
       " ⋅  ⋅  3  0  0  0\n",
       " ⋅  ⋅  ⋅  4  0  0\n",
       " ⋅  ⋅  ⋅  ⋅  5  0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using BandedMatrices\n",
    "BandedMatrix(-1=> 1:5, 2=>1:3)     # creates a 5 x 5 banded matrix version of diagm(-1=> 1:5, 2=>1:3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also a convenience function for generating random banded matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7×7 BandedMatrix{Float64,Array{Float64,2},Base.OneTo{Int64}}:\n",
       " 0.852039  0.909192   ⋅         ⋅         ⋅         ⋅         ⋅       \n",
       " 0.279266  0.365279  0.231345   ⋅         ⋅         ⋅         ⋅       \n",
       " 0.109687  0.719595  0.32635   0.374328   ⋅         ⋅         ⋅       \n",
       " 0.197615  0.441325  0.486003  0.386271  0.770824   ⋅         ⋅       \n",
       "  ⋅        0.999804  0.438857  0.424792  0.624999  0.177116   ⋅       \n",
       "  ⋅         ⋅        0.449552  0.754709  0.155209  0.916274  0.0422574\n",
       "  ⋅         ⋅         ⋅        0.815217  0.41038   0.13815   0.163309 "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = brand(7, 7, 3, 1)  # 3 subdiagonals, 1 subdiagonal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, of course, specialized algorithms will be used to exploit the structure when solving linear systems.  In particular, the complexity is related to the $ O(N P_L P_U) $ for upper and lower bandwidths $ P $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7-element Array{Float64,1}:\n",
       "  1.1696030461238491\n",
       " -0.5150413770678577\n",
       "  1.4940261680075608\n",
       "  1.9642389792846187\n",
       " -0.9650376463803029\n",
       " -0.8866876609401638\n",
       " -4.879284825844109 "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A \\ rand(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BlockBanded and BandedBlockBanded\n",
    "\n",
    "Taking the structured matrix concept further, we can consider examples of matrices in blocks, each of which are banded, and even\n",
    "a matrix where each block is banded, and the blocks themselves are banded.\n",
    "\n",
    "This final type is common in the discretization of multiple dimensions with continuous time processes.  For example, with the\n",
    "example from above of 2 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3-blocked 15×15 BandedBlockBandedMatrix{Float64,BlockArrays.PseudoBlockArray{Float64,2,Array{Float64,2},BlockArrays.BlockSizes{2,Tuple{Array{Int64,1},Array{Int64,1}}}}}:\n",
       " -0.25   0.05    ⋅      ⋅      ⋅    │  …    ⋅      ⋅     ⋅     ⋅     ⋅   │    ⋅ \n",
       "  0.05  -0.3    0.05    ⋅      ⋅    │       ⋅      ⋅     ⋅     ⋅     ⋅   │    ⋅ \n",
       "   ⋅     0.05  -0.3    0.05    ⋅    │       ⋅      ⋅     ⋅     ⋅     ⋅   │    ⋅ \n",
       "   ⋅      ⋅     0.05  -0.3    0.05  │      0.0     ⋅     ⋅     ⋅     ⋅   │    ⋅ \n",
       "   ⋅      ⋅      ⋅     0.05  -0.25  │\n",
       " ───────────────────────────────────┼      0.2     ⋅     ⋅     ⋅     ⋅   │    ⋅ \n",
       " ───────────────────────────────┼──────\n",
       "  0.1    0.0     ⋅      ⋅      ⋅    │  …    ⋅     0.2   0.0    ⋅     ⋅   │    ⋅ \n",
       "  0.0    0.1    0.0     ⋅      ⋅    │       ⋅     0.0   0.2   0.0    ⋅   │    ⋅ \n",
       "   ⋅     0.0    0.1    0.0     ⋅    │       ⋅      ⋅    0.0   0.2   0.0  │    ⋅ \n",
       "   ⋅      ⋅     0.0    0.1    0.0   │      0.15    ⋅     ⋅    0.0   0.2  │   0.0\n",
       "   ⋅      ⋅      ⋅     0.0    0.1   │\n",
       " ───────────────────────────────────┼     -0.45    ⋅     ⋅     ⋅    0.0  │   0.2\n",
       " ───────────────────────────────┼──────\n",
       "   ⋅      ⋅      ⋅      ⋅      ⋅    │  …    ⋅    -0.3   0.1    ⋅     ⋅   │    ⋅ \n",
       "   ⋅      ⋅      ⋅      ⋅      ⋅    │       ⋅     0.1  -0.4   0.1    ⋅   │    ⋅ \n",
       "   ⋅      ⋅      ⋅      ⋅      ⋅    │       ⋅      ⋅    0.1  -0.4   0.1  │    ⋅ \n",
       "   ⋅      ⋅      ⋅      ⋅      ⋅    │      0.0     ⋅     ⋅    0.1  -0.4  │   0.1\n",
       "   ⋅      ⋅      ⋅      ⋅      ⋅    │      0.1     ⋅     ⋅     ⋅    0.1  │  -0.3"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using BandedMatrices, BlockBandedMatrices, LazyArrays, SparseArrays\n",
    "function markov_chain_product_banded(Q_chains, A)\n",
    "    M = size(Q_chains[1], 1)\n",
    "    N = size(A, 1)\n",
    "    Q_bands = bandwidths(Q_chains[1])\n",
    "\n",
    "    Qs = blockdiag(sparse.(Q_chains)...)  # create diagonal blocks of every operator\n",
    "    Qs = BandedBlockBandedMatrix(Qs, (M*ones(Int64, N), M*ones(Int64, N)), (0,0), Q_bands)\n",
    "\n",
    "    # construct a kronecker product of A times I_M\n",
    "    As = BandedBlockBandedMatrix(Kron(A, Eye(M)))\n",
    "    return Qs + As\n",
    "end\n",
    "\n",
    "α1 = 0.05\n",
    "α2 = 0.15\n",
    "α3 = 0.1\n",
    "N = 5\n",
    "symmetric_tridiagonal_chain(α, N) = Tridiagonal(fill(α, N-1), [-α; fill(-2α, N-2); -α], fill(α, N-1))\n",
    "Q1 = symmetric_tridiagonal_chain(α1, N)\n",
    "Q2 = symmetric_tridiagonal_chain(α2, N)\n",
    "Q3 = symmetric_tridiagonal_chain(α3, N)\n",
    "A = Tridiagonal([0.1, 0.1], [-0.2, -0.3, -0.2], [0.2, 0.2])\n",
    "M = size(A,1)\n",
    "\n",
    "L = markov_chain_product_banded((Q1, Q2, Q3), A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, define a payoff function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15-element Array{Float64,1}:\n",
       "  3.0\n",
       "  4.0\n",
       "  5.0\n",
       "  6.0\n",
       "  7.0\n",
       "  5.0\n",
       "  6.0\n",
       "  7.0\n",
       "  8.0\n",
       "  9.0\n",
       "  7.0\n",
       "  8.0\n",
       "  9.0\n",
       " 10.0\n",
       " 11.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = vec([i + 2.0j for i in 1:N, j in 1:M])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving the equation $ \\rho v = r + L v $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×3 Array{Float64,2}:\n",
       "  69.7187   70.1489  58.1218\n",
       "  77.7164   76.5214  63.2774\n",
       "  88.2105   85.2632  70.1053\n",
       "  98.7046   94.0049  76.9331\n",
       " 106.702   100.377   82.0887"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ρ = 0.05\n",
    "v = (ρ * I - L) \\ r\n",
    "reshape(v, N, M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or to find the stationary solution,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×3 Array{Float64,2}:\n",
       " 0.0438447  0.0684658  0.0876894\n",
       " 0.0438447  0.0684658  0.0876894\n",
       " 0.0438447  0.0684658  0.0876894\n",
       " 0.0438447  0.0684658  0.0876894\n",
       " 0.0438447  0.0684658  0.0876894"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Arpack\n",
    "L = sparse(L')\n",
    "λ, ϕ = eigs(L, nev=1, which=:SM)\n",
    "ϕ = real(ϕ) ./ sum(real(ϕ))\n",
    "reshape(ϕ, N, M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id='implementation-numerics'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Low Level Kernels\n",
    "\n",
    "Recall the famous quote from Knuth: “Premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%”.  The most common example of premature optimization is trying to use your own mental model of a compiler while writing your code, overly worried about the efficiency of code and (usually incorrectly) second-guessing the compiler.\n",
    "\n",
    "Concretely, the lessons in this section are\n",
    "\n",
    "1. Don’t worry about optimizing your code unless you need to.  Code clarity is your first-order concern.  \n",
    "1. If you use other people’s packages, they can worry about performance and you don’t need to.  \n",
    "1. If you absolutely need that “critical 3%” your intuition about performance is usually wrong on modern CPUs and GPUs, so let the compiler do its job..  \n",
    "1. Benchmarking (e.g. `@btime`) and [profiling](https://docs.julialang.org/en/v1/manual/profile/) are the tools to figure out performance bottlenecks  \n",
    "1. If you benchmark to show that a particular part of the code is an issue, and you can’t find another library that does a better job, then you can worry about performance.  \n",
    "\n",
    "\n",
    "You will rarely get to step 3 let alone step 5.\n",
    "\n",
    "However, there is also a corollary:  “don’t pessimize prematurely”. That is, don’t make choices that lead to poor performance without any tradeoff in improved code clarity.  For example, writing your own algorithms when a high performance algorithm exists in a package or Julia itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation Difficulty\n",
    "\n",
    "Numerical analysis tend to refer to the lowest level of code for basic operations (e.g. a dot product, matrix-matrix product, convolutions) as `kernels`.\n",
    "\n",
    "That sort of code is difficult to write, and performance depends on the characteristics of the underlying hardware such as the [instruction set](https://en.wikipedia.org/wiki/Instruction_set_architecture) available on the particular CPU, the size of the [CPU cache](https://en.wikipedia.org/wiki/CPU_cache), the layout of arrays in memory, and a number of other things.\n",
    "\n",
    "Typically these operations are written in a [BLAS](https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms) library, organized into level.  The levels roughly correspond to the order of the opertions:  BLAS Level 1 are $ O(N) $ operations such as linear products, Level 2 are $ O(N^2) $ operations such as matrix-vector products, and Level 3 are $ O(N^3) $ such as matrix-matrix products.\n",
    "\n",
    "An example of a BLAS library is the [Intel MKL](https://en.wikipedia.org/wiki/Math_Kernel_Library) used in Matlab and Julia (if an additional package is installed.\n",
    "\n",
    "On top of BLAS are [LAPACK](https://en.wikipedia.org/wiki/LAPACK) operations, which are higher level kernels, such as Matrix factorizations and eigenvalue algorithms, and are often in the same libraries (e.g. MKL).\n",
    "\n",
    "The details of all of these are irrelevant, but if you are talking to people about performance, they will inevitably through out all of these concepts.  There are a few important lessons to keep in mind:\n",
    "\n",
    "- Leave writing kernels to the experts.  Even simple sounding algorithms can be very complicated to make high performance.  \n",
    "- Your intuition about performance of code is probably going to be wrong.  If you use high quality libraries, you don’t need to use your intuition.  \n",
    "- Don’t get distracted by the jargon or acronyms above if you are reading about performance.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Row and Column-Major Ordering\n",
    "\n",
    "Finally, there is one practical issue which may influence your code.  Since memory in a CPU is linear, dense matrices need to be stored by either stacking columns (called column-major order <https://en.wikipedia.org/wiki/Row-_and_column-major_order>) or by columns (called row-major order).\n",
    "\n",
    "The reason this matters is that compilers can generate better performance if they work in contiguous chunks of memory rather than jumping around, and this becomes especially important with large matrices due to the interaction with the CPU cache.  Choosing the wrong order when there is no benefit in code clarity is a classic example of premature pessimization.  The performance difference can be orders of magnitude in some cases, and nothing in other cases.\n",
    "\n",
    "One option is to use the functions that let the compiler choose the most efficient way to traverse memory. If you need to choose the looping order yourself, then you might want to experiment with swapping whether you go through columns or rows first.  Other times, let Julia decide, i.e. `enumerate` and `eachindex` will choose the right approach\n",
    "\n",
    "Julia, Fortran, and Matlab all use column-major order while C/C++ and Python use row-major order.  This means that if you find an algorithm written for C/C++/Python you will sometimes need to make small changes if performance is an issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "This exercise is for a little practice on low-level routines, and to hopefully convince you to leave low-level code to the experts.\n",
    "\n",
    "The formula for matrix multiplication is deceptively simple.  For example, with the product of square matrices $ C = A B $ of size $ N \\times N $, the $ i,j $ element of $ C $ is\n",
    "\n",
    "$$\n",
    "C_{ij} = \\sum_{k=1}^N A_{ik} B_{kj}\n",
    "$$\n",
    "\n",
    "Alternatively, you can take a row $ A_{i,:} $ and column $ B_{:, j} $ and use an inner product\n",
    "\n",
    "$$\n",
    "C_{ij} = A_{i,:} \\cdot B_{:,j}\n",
    "$$\n",
    "\n",
    "Note that the inner product in a discrete space is simply a sum, and has the same complexity (i.e. $ O(N) $ operations).\n",
    "\n",
    "For a dense matrix without any structure, this also makes it clear why the complexity is $ O(N^3) $: you need to evaluate it for $ N^2 $ elements in the matrix and do an $ O(N) $ operation each time.\n",
    "\n",
    "For this exercise, implement matrix multiplication yourself and compare performance in a few permutations.\n",
    "\n",
    "1. Use the built-in function in Julia (i.e.``C = A * B`` or, for a better comparison, the inplace version `mul!(C, A, B)` which works with preallocated data)  \n",
    "1. Loop over each $ C_{ij} $ by the row first (i.e. the `i` index) and use a `for` loop for the inner product  \n",
    "1. Loop over each $ C_{ij} $ by the column first (i.e. the `j` index) and use a `for` loop for the inner product  \n",
    "1. Do the same but use the `dot` product instead of the sum.  \n",
    "1. Choose your best implementation of these, and then for matrices of a few different sizes `N=10`, `N=1000`, etc. and compare the ratio of performance of your best implementation to the one built into Julia (actually, the BLAS library above).  \n",
    "\n",
    "\n",
    "A few more hints:\n",
    "\n",
    "- You can just use random matrices, e.g. `A = rand(N, N)`, etc.  \n",
    "- For all of them, preallocate the $ C $ matrix beforehand with `C = similar(A)` or something equivalent.  \n",
    "- To compare performance, put your code in a function and use `@btime` macro to time it.  Remember to escape globals if necessary (e.g. `@btime f(\\$A)` rather than `@btime f(A)`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2a\n",
    "\n",
    "Here we will calculate the evolution of a discrete time Markov Chain starting from :math:`phi_0`.\n",
    "\n",
    "Start with a simple symmetric tridiagonal matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100×100 Tridiagonal{Float64,Array{Float64,1}}:\n",
       " 0.8  0.1   ⋅    ⋅    ⋅    ⋅    ⋅    ⋅   …   ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅ \n",
       " 0.2  0.8  0.1   ⋅    ⋅    ⋅    ⋅    ⋅       ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅ \n",
       "  ⋅   0.1  0.8  0.1   ⋅    ⋅    ⋅    ⋅       ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅ \n",
       "  ⋅    ⋅   0.1  0.8  0.1   ⋅    ⋅    ⋅       ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅ \n",
       "  ⋅    ⋅    ⋅   0.1  0.8  0.1   ⋅    ⋅       ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅ \n",
       "  ⋅    ⋅    ⋅    ⋅   0.1  0.8  0.1   ⋅   …   ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅ \n",
       "  ⋅    ⋅    ⋅    ⋅    ⋅   0.1  0.8  0.1      ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅ \n",
       "  ⋅    ⋅    ⋅    ⋅    ⋅    ⋅   0.1  0.8      ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅ \n",
       "  ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅   0.1      ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅ \n",
       "  ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅       ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅ \n",
       "  ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅   …   ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅ \n",
       "  ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅       ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅ \n",
       "  ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅       ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅ \n",
       " ⋮                        ⋮              ⋱            ⋮                      \n",
       "  ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅       ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅ \n",
       "  ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅       ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅ \n",
       "  ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅   …   ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅ \n",
       "  ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅       ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅ \n",
       "  ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅      0.1   ⋅    ⋅    ⋅    ⋅    ⋅    ⋅ \n",
       "  ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅      0.8  0.1   ⋅    ⋅    ⋅    ⋅    ⋅ \n",
       "  ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅      0.1  0.8  0.1   ⋅    ⋅    ⋅    ⋅ \n",
       "  ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅   …   ⋅   0.1  0.8  0.1   ⋅    ⋅    ⋅ \n",
       "  ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅       ⋅    ⋅   0.1  0.8  0.1   ⋅    ⋅ \n",
       "  ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅       ⋅    ⋅    ⋅   0.1  0.8  0.1   ⋅ \n",
       "  ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅       ⋅    ⋅    ⋅    ⋅   0.1  0.8  0.2\n",
       "  ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅       ⋅    ⋅    ⋅    ⋅    ⋅   0.1  0.8"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 100\n",
    "A = Tridiagonal([fill(0.1, N-2); 0.2], fill(0.8, N), [0.2; fill(0.1, N-2)])\n",
    "A_adjoint = A'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Pick some large `T` and start with $ \\phi_0 = \\begin{bmatrix} 1 & 0 & \\ldots & 0\\end{bmatrix} $  \n",
    "1. Write code to calculate $ \\phi_t $ to some `T` by iterating the map for each `t`, i.e.  \n",
    "\n",
    "\n",
    "$$\n",
    "\\phi_{t+1} = A' \\phi_t\n",
    "$$\n",
    "\n",
    "1. What is the computational order of that calculating  $ \\phi_T $ using this iteration approach $ T < N $?  \n",
    "1. What is the computational order of $ (A')^T = (A \\ldots A) $ and then $ \\phi_T = (A')^T \\phi_0 $ for $ T < N $?  \n",
    "1. Benchmark calculating $ \\phi_T $ with the iterative calculation above as well as the direct $ \\phi_T = (A')^T \\phi_0 $ to see which is faster.  You can take the matrix power with just `A_adjoint^T`, uses specialized algorithms faster and more accurate than repeated matrix multiplication (but with the same computational order).  \n",
    "1. Check the same if $ T = 2 N $  \n",
    "\n",
    "\n",
    "Note: The algorithm used in Julia to take matrix powers  depends on the matrix structure, as usual.  In the symmetric case, it can use an eigendecomposition, whereas with a general dense matrix it uses [squaring and scaling](https://doi.org/10.1137/090768539)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2b\n",
    "\n",
    "With the same setup as Exercise 2a, do an [eigen decomposition](https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix) of `A_transpose`.  That is, use `eigen` to do a factorization of the adjoint $ A' = Q \\Lambda Q^{-1} $ where $ Q $ the matrix of eigenvectors and $ \\Lambda $ the diagonal matrix of eigenvalues.  Calculate $ Q^{-1} $ as well.\n",
    "\n",
    "> Use the factored matrix to calculate the sequence of $ \\phi_t = (A')^t \\phi_0 $ using the relationship\n",
    "\n",
    "\n",
    "$$\n",
    "\\phi_t = Q \\Lambda^t Q^{-1} \\phi_0\n",
    "$$\n",
    "\n",
    "Where matrix powers of diagonal matrices are simply the elementwise power of each element.\n",
    "\n",
    "Benchmark the speed of calculating the sequence of $ \\phi_t $ up to `T = 2N` using this method.  In principle, the factorization and easy calculation of the power should give you benefits.  Explain why it does or does not using computational order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "filename": "numerical_linear_algebra.rst",
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  },
  "title": "Numerical Linear Algebra and Factorizations"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
